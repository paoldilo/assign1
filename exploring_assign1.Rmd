---
title: "Exploring Capstone Dataset"
author: "Paolo Di Lorenzo"
date: "June 12, 2016"
output: html_document
---


## Introduction and File Loading

The document describes the analysis performed on the dataset providec for the Capstone project that is made of 3 files:   

* The first one is a collection from blog articles in english
* The second one is a collection from news articles in english
* The third one is a collection from twitter posts in english

   
Files are quite big (more than 500Mb) and could not be handled and analyzed quickly with normal computing processing power, so some kind of sampling will be performed as will be explained later on.
The number of lines in each files are:   
   
```{r echo=TRUE,message=FALSE,cache=TRUE}

library(Rgraphviz)
library(R.utils)
library(tm)
countLines("./en_US/en_US.blogs.txt")[1]
countLines("./en_US/en_US.news.txt")[1]
countLines("./en_US/en_US.twitter.txt")[1]
```

   
   The next chunk of code will execute sampling of the original files. The steps are performed as follows:   

1. Original file is read from file system   
2. I sample the lines of the original file using a binomial probability distribution with a 5% sampling rate (that is I take 1/20th of the original file)
3. I write back the file to file system with an appendix "reduced" in the filename so I could filter it up later on
4. I remove old variables because they consume large amounts of memory

   
```{r echo=TRUE,message=FALSE,cache=TRUE,warning=FALSE}

set.seed(5150)
percentage <- .05
## sample blogs at text percentage
original <- file("./en_US/en_US.blogs.txt","r")
file_blog <- readLines(original)
reduced <- file_blog[rbinom(n = length(file_blog), size = 1, prob = percentage) == 1]
close(original)
conn <- file("./en_US/en_US.blogs_reduced.txt", "w")
writeLines(reduced, con = conn)
close(conn)
## sample news at text percentage
original <- file("./en_US/en_US.news.txt","r")
file_news <- readLines(original)
reduced <- file_news[rbinom(n = length(file_news), size = 1, prob = percentage) == 1]
close(original)
conn <- file("./en_US/en_US.news_reduced.txt", "w")
writeLines(reduced, con = conn)
close(conn)
## sample twitter at text percentage
original <- file("./en_US/en_US.twitter.txt","r")
file_twit <- readLines(original)
reduced <- file_twit[rbinom(n = length(file_twit), size = 1, prob = percentage) == 1]
close(original)
conn <- file("./en_US/en_US.twitter_reduced.txt", "w")
writeLines(reduced, con = conn)
close(conn)
#save memory resources
rm(file_news)
rm(file_blog)
rm(file_twit)
rm(reduced)
```

   The new files are more manageable having fewer lines:

```{r echo=TRUE,message=FALSE,cache=TRUE}
countLines("./en_US/en_US.blogs_reduced.txt")[1]
countLines("./en_US/en_US.news_reduced.txt")[1]
countLines("./en_US/en_US.twitter_reduced.txt")[1]
```

## Creating the Corpus and performing Data Cleaning

Once a new subsampled dataset has been created the next step is loading into memory as a large collection of text (also known as Corpus).
The Corpus needs some cleaning since there are a lot of unusable characters for text prediction, so I use the 'tm' package built-in functions to remove:
* Punctuation
* Useless whitespaces
* Transform all to lowercase so words can be compared
* Remove numbers that cannot count as words (this caould be arguable but at the moment I can not evaluate all 4 as the word "for", I would have to evaluate the semantics of every sentence)
   I finally obtain a small and manageable dataset (almost 40Mb).   

```{r echo=TRUE,message=FALSE,cache=TRUE}
# create a new reduced corpus
mainCorpus<-Corpus(DirSource("./en_US/",pattern = "reduced"))
# clean the dataset
mainCorpus<-tm_map(mainCorpus,removePunctuation)
mainCorpus<-tm_map(mainCorpus, stripWhitespace)
mainCorpus<-tm_map(mainCorpus, content_transformer(tolower))
mainCorpus<-tm_map(mainCorpus,removeNumbers)
object.size(mainCorpus)
```
   
## Exploratory Data Analysis

```{r echo=TRUE,message=FALSE,cache=TRUE}
# create a term document matrix including stopwords
TDM_stopwords <- TermDocumentMatrix(mainCorpus, control = list(stopwords = FALSE))
limit <- 5000
# find the most used words
findFreqTerms(TDM_stopwords, limit, Inf)
m_stop <- as.matrix(TDM_stopwords)
v_stop <- sort(rowSums(m_stop), decreasing=TRUE)
#plot a histogram of the most used words
plot(head(v_stop, 15),type = 'h',main = "Most frequent words including Stopwords")
# plot the correlation matrix of the 20 most used words
plot(TDM_stopwords,terms=names(v_stop[1:20]), corThreshold = 0.8, weighting = F)
```

```{r echo=TRUE,message=FALSE,cache=TRUE}
# create a term document matrix including stopwords
TDM_nostopwords <- TermDocumentMatrix(mainCorpus, control = list(stopwords = TRUE))
# find the most used words
findFreqTerms(TDM_nostopwords, limit, Inf)
m_nostop <- as.matrix(TDM_nostopwords)
v_nostop <- sort(rowSums(m_nostop), decreasing=TRUE)
#plot a histogram of the most used words
plot(head(v_nostop, 15),type = 'h',main = "Most frequent words excluding Stopwords")
# plot the correlation matrix of the 20 most used words
plot(TDM_nostopwords,terms=names(v_nostop[1:25]), corThreshold = 0.8, weighting = F)
```